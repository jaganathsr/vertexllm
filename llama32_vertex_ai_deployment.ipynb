{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Llama 3.2 on Vertex AI with Custom Container\n",
    "\n",
    "This notebook provides a complete walkthrough for deploying Meta's Llama 3.2 model on Google Cloud Vertex AI using a custom container image.\n",
    "\n",
    "## Prerequisites\n",
    "- Google Cloud Project with billing enabled\n",
    "- Vertex AI API enabled\n",
    "- Access to Llama 3.2 model weights (from Meta or Hugging Face)\n",
    "- Docker installed locally (for building the container)\n",
    "- `gcloud` CLI configured\n",
    "\n",
    "## Overview\n",
    "1. Set up environment and dependencies\n",
    "2. Create custom container with model serving code\n",
    "3. Build and push container to Google Container Registry\n",
    "4. Upload model to Vertex AI Model Registry\n",
    "5. Deploy to endpoint\n",
    "6. Test the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-cloud-aiplatform transformers torch accelerate sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Google Cloud project variables\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your project ID\n",
    "REGION = \"us-central1\"  # Choose your preferred region\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-llama-models\"  # GCS bucket for model artifacts\n",
    "REPOSITORY = \"llama-models\"  # Artifact Registry repository name\n",
    "IMAGE_NAME = \"llama32-serve\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"llama-3.2-3b\"  # or llama-3.2-1b, llama-3.2-11b, llama-3.2-90b\n",
    "HUGGINGFACE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"  # Update based on your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create GCS Bucket and Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GCS bucket for model artifacts\n",
    "!gsutil mb -l {REGION} gs://{BUCKET_NAME} || echo \"Bucket already exists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Artifact Registry repository\n",
    "!gcloud artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location={REGION} \\\n",
    "    --description=\"Repository for Llama models\" || echo \"Repository already exists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Docker to authenticate with Artifact Registry\n",
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Custom Container Files\n",
    "\n",
    "We'll create the necessary files for our custom prediction container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(\"container\", exist_ok=True)\n",
    "os.chdir(\"container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY predictor.py .\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV AIP_HTTP_PORT=8080\n",
    "ENV AIP_HEALTH_ROUTE=/health\n",
    "ENV AIP_PREDICT_ROUTE=/predict\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Run the web service\n",
    "CMD [\"python\", \"predictor.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers>=4.38.0\n",
    "torch>=2.1.0\n",
    "accelerate>=0.25.0\n",
    "sentencepiece>=0.1.99\n",
    "protobuf>=3.20.0\n",
    "flask>=3.0.0\n",
    "gunicorn>=21.2.0\n",
    "google-cloud-storage>=2.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictor.py - Main Serving Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predictor.py\n",
    "import os\n",
    "import logging\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global variables for model and tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "# Environment variables\n",
    "AIP_HTTP_PORT = os.getenv(\"AIP_HTTP_PORT\", \"8080\")\n",
    "AIP_HEALTH_ROUTE = os.getenv(\"AIP_HEALTH_ROUTE\", \"/health\")\n",
    "AIP_PREDICT_ROUTE = os.getenv(\"AIP_PREDICT_ROUTE\", \"/predict\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"/models\")\n",
    "HUGGINGFACE_MODEL_ID = os.getenv(\"HUGGINGFACE_MODEL_ID\", \"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)  # Hugging Face token for gated models\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the Llama model and tokenizer\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    logger.info(f\"Loading model from {HUGGINGFACE_MODEL_ID}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            HUGGINGFACE_MODEL_ID,\n",
    "            token=HF_TOKEN,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load model with appropriate device settings\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            HUGGINGFACE_MODEL_ID,\n",
    "            token=HF_TOKEN,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if device == \"cpu\":\n",
    "            model = model.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@app.route(AIP_HEALTH_ROUTE, methods=[\"GET\"])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({\"status\": \"healthy\"}), 200\n",
    "\n",
    "@app.route(AIP_PREDICT_ROUTE, methods=[\"POST\"])\n",
    "def predict():\n",
    "    \"\"\"Prediction endpoint\"\"\"\n",
    "    try:\n",
    "        # Parse request\n",
    "        request_json = request.get_json()\n",
    "        instances = request_json.get(\"instances\", [])\n",
    "        \n",
    "        if not instances:\n",
    "            return jsonify({\"error\": \"No instances provided\"}), 400\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for instance in instances:\n",
    "            # Extract prompt and parameters\n",
    "            prompt = instance.get(\"prompt\", \"\")\n",
    "            max_tokens = instance.get(\"max_tokens\", 512)\n",
    "            temperature = instance.get(\"temperature\", 0.7)\n",
    "            top_p = instance.get(\"top_p\", 0.9)\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Remove the input prompt from the output\n",
    "            response = generated_text[len(prompt):].strip()\n",
    "            \n",
    "            predictions.append({\n",
    "                \"generated_text\": response,\n",
    "                \"full_output\": generated_text\n",
    "            })\n",
    "        \n",
    "        return jsonify({\"predictions\": predictions}), 200\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model at startup\n",
    "    load_model()\n",
    "    \n",
    "    # Run Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=int(AIP_HTTP_PORT), debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and Push Container Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Docker image\n",
    "!docker build -t {IMAGE_URI} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the image to Artifact Registry\n",
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to parent directory\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Upload Model to Vertex AI\n",
    "\n",
    "**Important:** If you're using a gated model from Hugging Face (like Llama), you'll need to:\n",
    "1. Request access on the Hugging Face model page\n",
    "2. Create a Hugging Face access token\n",
    "3. Pass it as an environment variable during deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set your Hugging Face token if needed\n",
    "HF_TOKEN = \"your-huggingface-token\"  # Replace with your token or leave empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Define environment variables for the container\n",
    "env_vars = {\n",
    "    \"HUGGINGFACE_MODEL_ID\": HUGGINGFACE_MODEL_ID,\n",
    "}\n",
    "\n",
    "if HF_TOKEN and HF_TOKEN != \"your-huggingface-token\":\n",
    "    env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# Upload model to Vertex AI Model Registry\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=f\"{MODEL_NAME}-custom\",\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_environment_variables=env_vars,\n",
    "    serving_container_ports=[8080],\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\",\n",
    ")\n",
    "\n",
    "print(f\"Model uploaded with resource name: {model.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy Model to Endpoint\n",
    "\n",
    "Choose appropriate machine type based on your model size:\n",
    "- **Llama 3.2 1B/3B**: `n1-standard-4` or `n1-standard-8`\n",
    "- **Llama 3.2 11B**: `n1-standard-8` or `n1-highmem-8` with GPU\n",
    "- **Llama 3.2 90B**: GPU required (e.g., `n1-standard-16` with A100 or V100)\n",
    "\n",
    "For GPU deployment, you'll need to request GPU quota and modify the machine type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"{MODEL_NAME}-endpoint\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "print(f\"Endpoint created: {endpoint.display_name}\")\n",
    "print(f\"Endpoint resource name: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to endpoint\n",
    "# This can take 10-20 minutes\n",
    "MACHINE_TYPE = \"n1-standard-8\"  # Adjust based on your model size\n",
    "\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=f\"{MODEL_NAME}-deployment\",\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    traffic_percentage=100,\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Deploy with GPU\n",
    "\n",
    "For larger models, use GPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to deploy with GPU\n",
    "# MACHINE_TYPE = \"n1-standard-16\"\n",
    "# ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"  # or NVIDIA_TESLA_A100\n",
    "# ACCELERATOR_COUNT = 1\n",
    "\n",
    "# deployed_model = model.deploy(\n",
    "#     endpoint=endpoint,\n",
    "#     deployed_model_display_name=f\"{MODEL_NAME}-deployment\",\n",
    "#     machine_type=MACHINE_TYPE,\n",
    "#     accelerator_type=ACCELERATOR_TYPE,\n",
    "#     accelerator_count=ACCELERATOR_COUNT,\n",
    "#     min_replica_count=1,\n",
    "#     max_replica_count=1,\n",
    "#     traffic_percentage=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "test_instance = {\n",
    "    \"prompt\": \"Write a short poem about artificial intelligence:\",\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "prediction = endpoint.predict(instances=[test_instance])\n",
    "print(\"\\nPrediction result:\")\n",
    "print(prediction.predictions[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a conversational prompt\n",
    "conversation_instance = {\n",
    "    \"prompt\": \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful AI assistant.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What are the three most important things to know about machine learning?<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "prediction = endpoint.predict(instances=[conversation_instance])\n",
    "print(\"\\nConversational prediction:\")\n",
    "print(prediction.predictions[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Using the Endpoint via REST API\n",
    "\n",
    "You can also call the endpoint using REST API from any application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# Get authentication credentials\n",
    "credentials, project = default()\n",
    "credentials.refresh(Request())\n",
    "\n",
    "# Prepare the API endpoint URL\n",
    "endpoint_id = endpoint.name.split('/')[-1]\n",
    "api_endpoint = f\"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}:predict\"\n",
    "\n",
    "# Prepare request\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {credentials.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"prompt\": \"Explain quantum computing in simple terms:\",\n",
    "            \"max_tokens\": 150,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Make request\n",
    "response = requests.post(api_endpoint, headers=headers, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "print(\"\\nREST API Response:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Monitoring and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all endpoints\n",
    "endpoints = aiplatform.Endpoint.list()\n",
    "print(\"Available endpoints:\")\n",
    "for ep in endpoints:\n",
    "    print(f\"- {ep.display_name}: {ep.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get endpoint details\n",
    "print(f\"\\nEndpoint details:\")\n",
    "print(f\"Display name: {endpoint.display_name}\")\n",
    "print(f\"Resource name: {endpoint.resource_name}\")\n",
    "print(f\"Create time: {endpoint.create_time}\")\n",
    "print(f\"\\nDeployed models:\")\n",
    "for deployed_model in endpoint.gca_resource.deployed_models:\n",
    "    print(f\"- {deployed_model.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Cleanup (Optional)\n",
    "\n",
    "**Warning:** Running these cells will delete your endpoint and model. Only run if you want to clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy model from endpoint\n",
    "# endpoint.undeploy_all()\n",
    "# print(\"All models undeployed from endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "# endpoint.delete(force=True)\n",
    "# print(\"Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "# model.delete()\n",
    "# print(\"Model deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Custom Container Serving](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)\n",
    "- [Llama 3.2 Model Card](https://huggingface.co/meta-llama)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "\n",
    "## Tips and Best Practices\n",
    "\n",
    "1. **Model Size**: Choose the appropriate model size based on your use case. Smaller models (1B, 3B) are faster and cheaper.\n",
    "2. **GPU vs CPU**: For production workloads with larger models, GPUs are recommended for better performance.\n",
    "3. **Autoscaling**: Configure min/max replicas based on expected traffic patterns.\n",
    "4. **Monitoring**: Set up Cloud Monitoring alerts for endpoint health and latency.\n",
    "5. **Cost Optimization**: Use Spot VMs or preemptible instances for development/testing.\n",
    "6. **Security**: Store HF tokens in Secret Manager instead of environment variables for production.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Issue**: Container fails to start\n",
    "- Check Cloud Logging for container logs\n",
    "- Verify Hugging Face token is valid\n",
    "- Ensure sufficient memory/CPU for model size\n",
    "\n",
    "**Issue**: Slow predictions\n",
    "- Consider using GPU instances\n",
    "- Enable model quantization (8-bit or 4-bit)\n",
    "- Use smaller models for faster inference\n",
    "\n",
    "**Issue**: Out of memory errors\n",
    "- Increase machine type memory\n",
    "- Reduce max_tokens in requests\n",
    "- Enable gradient checkpointing in model loading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
